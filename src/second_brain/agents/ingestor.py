from pathlib import Path
from sentence_transformers import SentenceTransformer
import chromadb
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- Global model and client setup ---
client = chromadb.PersistentClient(path=".chromadb")
collection = client.get_or_create_collection(name="notes")
model = SentenceTransformer("all-MiniLM-L6-v2")

# --- Helper for embedding ---
def embed_text(text: str):
    return model.encode(text).tolist()

# --- Ingest logic with chunking ---
def ingest_folder(folder_path: str = "data/notes"):
    folder = Path(folder_path)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    for file_path in folder.glob("*.txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        chunks = splitter.split_text(content)
        print(f"ðŸ“„ Splitting {file_path.name} into {len(chunks)} chunks...")

        for i, chunk in enumerate(chunks):
            doc_id = f"{file_path.stem}_{i}"
            embedding = embed_text(chunk)

            collection.add(
                ids=[doc_id],
                embeddings=[embedding],
                documents=[chunk],
                metadatas=[{
                    "filename": file_path.name,
                    "chunk_index": i
                }],
            )

        print(f"âœ… Ingested {file_path.name} ({len(chunks)} chunks)")

# --- Reset collection ---
def reset_collection():
    """Deletes the ChromaDB collection for a clean start."""
    try:
        client.delete_collection("notes")
        print("ðŸ§¹ Collection 'notes' deleted successfully.")
    except Exception as e:
        print(f"âš ï¸ No existing collection found or error deleting: {e}")

# --- Query logic ---
def query_notes(query: str, n_results: int = 3):
    query_embedding = embed_text(query)

    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )

    docs = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]

    if not docs:
        print("âŒ No matching results found.")
        return

    seen = set()
    for doc, meta in zip(docs, metas):
        key = f"{meta['filename']}_{meta['chunk_index']}"
        if key in seen:
            continue
        seen.add(key)

        print(f"\nðŸ“˜ {meta['filename']}:")
        print(doc[:400] + "\n---")

    return results


# --- RAG retrieval ---
def rag_retrieve(query: str, n_results: int = 3) -> str:
    """
    Retrieve top-n relevant chunks from ChromaDB and prepare a clean context block
    for use in RAG pipelines (Retrieval-Augmented Generation).
    """
    query_embedding = embed_text(query)

    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )

    if not results["documents"] or not results["documents"][0]:
        return "No relevant information found in your knowledge base."

    # Combine retrieved documents into a readable context string
    context_blocks = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        filename = meta.get("filename", "Unknown file")
        chunk_info = f"[Source: {filename}]"
        context_blocks.append(f"{chunk_info}\n{doc.strip()}\n")

    context = "\n---\n".join(context_blocks)

    final_context = f"""Here are some relevant notes from your knowledge base:
    
                    {context}
                    """

    return final_context

